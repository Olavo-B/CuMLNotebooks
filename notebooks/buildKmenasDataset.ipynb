{"cells":[{"cell_type":"markdown","metadata":{"id":"Tvmo1P3bByN_"},"source":["## Dependences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jWe9syhAnnJ-"},"outputs":[],"source":["import warnings\n","import time\n","\n","# Suprimir todos os avisos\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","metadata":{"id":"riCiVJSFBff1"},"source":["## Load Datasets & Utils Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"riKlSp8pjxDD"},"outputs":[],"source":["def write_dict_to_csv(filename, data_dict):\n","    import os\n","    import csv\n","    # Check if the file exists\n","    file_exists = os.path.exists(filename)\n","\n","    # Open the CSV file in append mode so that existing data isn't overwritten\n","    with open(filename, mode='a', newline='') as file:\n","        fieldnames = data_dict.keys()\n","        writer = csv.DictWriter(file, fieldnames=fieldnames)\n","\n","        # If the file doesn't exist, write the header\n","        if not file_exists:\n","            writer.writeheader()\n","\n","        # Write the dictionary as a new row\n","        writer.writerow(data_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KVH1eRUWhm3I"},"outputs":[],"source":["def time_stamp():\n","    import time\n","    return time.strftime(\"%H%M_%d%m%Y\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ofUzz7fTQRZx"},"outputs":[],"source":["def load_susy(debug=False):\n","    \"\"\"Load the SUSY dataset into data and target dictionaries\"\"\"\n","\n","    import requests\n","    import zipfile\n","    import io\n","    import pandas as pd\n","    from tqdm import tqdm\n","    import gzip\n","\n","    url = 'https://archive.ics.uci.edu/static/public/279/susy.zip'\n","    response = requests.get(url, stream=True)\n","\n","    if response.status_code == 200:\n","        total_size = int(response.headers.get('content-length', 0))\n","        block_size = 1024\n","        t = tqdm(total=total_size, unit='iB', unit_scale=True)\n","\n","        file_buffer = io.BytesIO()\n","        for data in response.iter_content(block_size):\n","            t.update(len(data))\n","            file_buffer.write(data)\n","        t.close()\n","\n","        file_buffer.seek(0)\n","\n","        with zipfile.ZipFile(file_buffer) as the_zip:\n","            with the_zip.open('SUSY.csv.gz') as gz_file:\n","                with gzip.open(gz_file) as the_file:\n","                    # Load the full CSV into a DataFrame\n","                    the_file.seek(0)\n","                    df = pd.read_csv(the_file, names=[str(i) for i in range(0,19)])\n","\n","                    # Rename the first column to 'target' and separate it as a Series\n","                    target = df.iloc[:, 0]\n","                    target.name = 'target'\n","\n","                    # Get the remaining columns as a DataFrame\n","                    data = df.iloc[:, 1:]\n","\n","                    if debug:\n","                        print(\"===========LOAD DATASET============\")\n","                        print(\"Data list shape: \", data.shape)\n","                        print(data.head())\n","\n","\n","                    return data, target\n","\n","    else:\n","        print(f\"Failed to download the file. Status code: {response.status_code}\")\n","        return None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lvkimv5oQW2L"},"outputs":[],"source":["def load_data(data, debug=False):\n","    \"\"\"\n","    Load the dataset into data and target dictionaries\n","\n","    Parameters\n","    ----------\n","\n","    data : str\n","        Path to the data\n","\n","\n","    \"\"\"\n","    data = pd.read_csv(data, header=None, sep = ' ')\n","\n","    if debug:\n","        print(\"===========LOAD DATA============\")\n","        print(\"Data list shape: \", data.shape)\n","        print(data.head())\n","\n","    return data"]},{"cell_type":"markdown","metadata":{"id":"FqSmw-oLBltH"},"source":["## Build Dataset Class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LV30cWDU4PEX"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","\n","\n","\n","class make_kmeans_dataset():\n","\n","    def __init__(self, data, thread_list, dim, k):\n","        self.data = data\n","        self.thread_list = thread_list\n","        self.dim = dim\n","        self.k = k\n","\n","\n","\n","\n","    def get_cluster(self,att: list, centroids: list = None ,):\n","        \"\"\"Give a list of centroids points and  a list of values, return the label from\n","        what cluster the data is. Uses numpy for faster computation\n","\n","            Parameters\n","            ----------\n","\n","            centroids : list\n","                List of centroids points\n","                [centroid1, centroid2, centroid3, centroid4, ...]\n","                Note that centroid1, centroid2, centroid3 can be a point in space (if dim == 3)\n","                that will be used to get the distance between the data_values and the centroids\n","\n","            att: list\n","                A list of selected attribuites that will be used to generate the\n","                new coloumn\n","\n","            Returns\n","            -------\n","            cluster : list\n","                The index of the cluster that eand point in data_values belongs to\n","\n","        \"\"\"\n","\n","        data_values = self.data[att]\n","\n","\n","        # Reshape the centroids to a 2D array with dim columns\n","        if centroids is not None:\n","            centroids = np.array(centroids).reshape(-1, data_values.shape[1])\n","        else:\n","            raise ValueError(\"Centroids is None\")\n","\n","        # Convert data_values to a CuPy array\n","        data_values = np.array(data_values.values)\n","\n","        # Get the distance between the data_values and\n","        # the centroids\n","        # Calculate the distance between the data_values and the centroids\n","        distances = np.linalg.norm(data_values[:, None] - centroids, axis=2)\n","\n","        # Get the index of the minimum distance for each point\n","        clusters = np.argmin(distances, axis=1)\n","\n","        return clusters.get().tolist()  # Convert the CuPy array back to a list for compatibility\n","\n","    def select_rows(self, n_rows: int = 1, method = 'random' , row_id=None, debug=False):\n","        \"\"\"\n","        Select the rows that will be used to create the new columns\n","\n","        Parameters\n","        ----------\n","\n","        n_rows : int = 1\n","            The number of rows that will be selected\n","\n","        debug : bool = False\n","            If True, print debug information\n","\n","        row_id : list = None\n","            The row id that will be used to create the new columns\n","            If None, a random row will be selected\n","\n","        Returns\n","        -------\n","        thread_row : pd.DataFrame\n","            The selected rows\n","\n","        row_id : list\n","            The row id that was used to create the new columns\n","\n","        \"\"\"\n","\n","        if row_id is None:\n","            if method == 'random':\n","                thread_row = self.thread_list.sample(n=n_rows)\n","                while thread_row.iloc[:, 0:self.dim].duplicated().any():\n","                    thread_row = self.thread_list.sample(n=n_rows)\n","\n","            elif method == 'sequential':\n","                thread_row = self.thread_list.iloc[0:n_rows]\n","\n","            elif method == 'gini':\n","                # Get the n_rows rows with the smallest sum of gini (last 8 columns)\n","                thread_row = self.thread_list.nsmallest(n_rows, self.thread_list.columns[-self.k:])\n","                while thread_row.iloc[:, 0:self.dim].duplicated().any():\n","                    thread_row = self.thread_list.nsmallest(n_rows, self.thread_list.columns[-self.k:])\n","\n","            row_id = thread_row.index.to_arrow().to_pylist()\n","\n","\n","\n","        else:\n","            thread_row = self.thread_list.iloc[row_id]\n","\n","        if debug:\n","            print(\"Row id: \", row_id)\n","            print(\"=====================================\")\n","\n","        return thread_row, row_id\n","\n","\n","\n","    def build_new_columns(self, n_columns: int =1, row_id = None, method = 'random', debug=False):\n","        \"\"\"Build the new columns for the dataset\n","\n","        Parameters\n","        ----------\n","\n","        n_columns : int = 1\n","            The number of columns that will be created\n","\n","        row_id : list = None\n","            The row id that will be used to create the new columns\n","            If None, a random row will be selected\n","\n","        debug : bool = False\n","            If True, print debug information\n","\n","        Returns\n","        -------\n","        new_data : pd.DataFrame\n","            The new dataset with the new columns\n","\n","        row_id : list\n","            The row id that was used to create the new columns\n","\n","\n","\n","\n","        \"\"\"\n","\n","        new_data   = pd.DataFrame()\n","        thread_row, row_id = self.select_rows(n_columns, debug=debug,\n","                                              row_id=row_id, method = method)\n","\n","        if debug:\n","            print(\"============BUILD COLUMN=========\")\n","            print(thread_row)\n","\n","        for i in range(n_columns):\n","            # Get the centroids\n","            centroids = thread_row.iloc[i, self.dim:self.dim+(self.dim*self.k)].values.tolist()\n","            # Get the used attributes\n","            att       = [str(i) for i in thread_row.iloc[i, 0:self.dim].values.tolist()]\n","\n","            if debug:\n","                print(\"Centroids: \", centroids)\n","                print(\"Attributes: \", att)\n","\n","            labels = self.get_cluster(att, centroids)\n","\n","            # Create the new columns\n","            new_data[f'cluster_{i}'] = labels\n","\n","\n","        return new_data,row_id\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DvG2VFTlBpXk"},"source":["## Import Data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ki7F4TmSbQyu","outputId":"5ee5c471-e943-466a-caa4-a5acd858ada3"},"outputs":[{"name":"stderr","output_type":"stream","text":["922MiB [03:26, 4.46MiB/s] \n"]},{"name":"stdout","output_type":"stream","text":["===========LOAD DATASET============\n","Data list shape:  (5000000, 18)\n","          1         2         3         4         5         6         7  \\\n","0  0.972861  0.653855  1.176225  1.157156 -1.739873 -0.874309  0.567765   \n","1  1.667973  0.064191 -1.225171  0.506102 -0.338939  1.672543  3.475464   \n","2  0.444840 -0.134298 -0.709972  0.451719 -1.613871 -0.768661  1.219918   \n","3  0.381256 -0.976145  0.693152  0.448959  0.891753 -0.677328  2.033060   \n","4  1.309996 -0.690089 -0.676259  1.589283 -0.693326  0.622907  1.087562   \n","\n","          8         9        10        11        12        13        14  \\\n","0 -0.175000  0.810061 -0.252552  1.921887  0.889637  0.410772  1.145621   \n","1 -1.219136  0.012955  3.775174  1.045977  0.568051  0.481928  0.000000   \n","2  0.504026  1.831248 -0.431385  0.526283  0.941514  1.587535  2.024308   \n","3  1.533041  3.046260 -1.005285  0.569386  1.015211  1.582217  1.551914   \n","4 -0.381742  0.589204  1.365479  1.179295  0.968218  0.728563  0.000000   \n","\n","         15        16        17        18  \n","0  1.932632  0.994464  1.367815  0.040714  \n","1  0.448410  0.205356  1.321893  0.377584  \n","2  0.603498  1.562374  1.135454  0.180910  \n","3  0.761215  1.715464  1.492257  0.090719  \n","4  1.083158  0.043429  1.154854  0.094859  \n"]}],"source":["# Load the dataset\n","# thread_list = load_data('./misc/data/Kmedoids10k.csv', debug=True)\n","data,target = load_susy(debug = True)\n","target = target.astype('float32')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b4_3ecC9nnKN","outputId":"9a644153-f0e8-4a83-8ca4-945fac45b66b"},"outputs":[{"name":"stdout","output_type":"stream","text":["===========LOAD DATA============\n","Data list shape:  (10240, 35)\n","   0   1   2    3    4    5         6         7         8         9   ...  \\\n","0  14   5  15  0.0  0.0  0.0  0.525004 -1.337430  0.969126  0.237978  ...   \n","1   2  17  14  0.0  0.0  0.0 -0.958958  1.215950  0.364479  0.509009  ...   \n","2   6   1   8  0.0  0.0  0.0 -0.571164  1.226540 -1.139080  1.083150  ...   \n","3  14   9  15  0.0  0.0  0.0  0.634961  0.783559  2.415120  1.645030  ...   \n","4  11   1  18  0.0  0.0  0.0  0.000000  0.000000  0.000000  2.612770  ...   \n","\n","         25        26   27        28        29        30        31        32  \\\n","0 -0.849737  0.909070  0.0  0.459118  0.491110  0.433444  0.493343  0.417547   \n","1  1.043310  1.893040  0.0  0.490649  0.498953  0.480121  0.497741  0.498214   \n","2  0.844672  0.940628  0.0  0.498010  0.479752  0.483604  0.499915  0.493842   \n","3  0.266001  0.501657  0.0  0.470673  0.014858  0.429709  0.495462  0.436205   \n","4  1.099090  0.258224  0.0  0.000000  0.294411  0.000000  0.445393  0.296207   \n","\n","         33        34  \n","0  0.499909  0.498834  \n","1  0.488142  0.499982  \n","2  0.485052  0.480611  \n","3  0.495604  0.357208  \n","4  0.454513  0.471946  \n","\n","[5 rows x 35 columns]\n"]}],"source":["thread_list = load_data('./misc/data/kmeans10k.csv', debug=True)\n","target = target.astype('float32')"]},{"cell_type":"markdown","metadata":{"id":"SgDbr5zxnnKO"},"source":["## Build Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t9jFDyonnnKO"},"outputs":[],"source":["# Criação do objeto kmeans (ou outro método, dependendo do JSON)\n","_k, _dim = 8,3\n","# Verificando se as colunas em thread_list 1,2,3 são iguais a 0,6,14\n","_row_id = thread_list[(thread_list[0] == 0) & (thread_list[1] == 6) & (thread_list[2] == 14)].index.to_list()\n","_debug = False\n","kmeans = make_kmeans_dataset(data, thread_list, _k, _dim)\n","\n","new_data, index = kmeans.build_new_columns(n_columns=3, row_id=_row_id,\n","                                               method='random', debug=_debug)"]},{"cell_type":"markdown","metadata":{"id":"eBNAcYgYfksh"},"source":["## Run RandomForest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IIy2WQRPVX2w"},"outputs":[],"source":["def run_random_forest(new_data, target, _n_estimators, _max_depth, debug=False):\n","      from cuml.ensemble import RandomForestClassifier\n","      from cuml.metrics import accuracy_score\n","      from cuml.model_selection import train_test_split\n","      import cudf\n","\n","        # Converter os dados para um DataFrame do cudf\n","      X_cudf = new_data\n","      y_cudf = cudf.Series(target)\n","\n","      # Dividir os dados em conjuntos de treinamento e teste\n","      X_train, X_test, y_train, y_test = train_test_split(X_cudf, y_cudf, test_size=0.2, random_state=42)\n","\n","\n","      # Instanciar e treinar o modelo RandomForest\n","      rf = RandomForestClassifier(n_estimators=_n_estimators,\n","                                  max_depth = _max_depth,\n","                                  random_state=42)\n","      rf.fit(X_train, y_train)\n","\n","      # Fazer previsões no conjunto de teste\n","      y_pred = rf.predict(X_test)\n","\n","      # Calcular a acurácia\n","      accuracy = accuracy_score(y_test, y_pred)\n","      if debug:\n","        print(f'Acurácia do modelo RandomForest {_n_estimators},{_max_depth}: {accuracy}')\n","\n","      return accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Wbhu1reVKRb"},"outputs":[],"source":["# Carregando o JSON\n","import json\n","with open('config.json','r') as file:\n","    config_json = file.read()\n","\n","config = json.loads(config_json)\n","\n","# Extraindo os parâmetros do JSON\n","_id_start            = config['id_start']\n","_k                   = config['k']\n","_dim                 = config['dim']\n","_num_bench           = config['num_bench']\n","_n_columns_benchmark = (config['n_coluns']*_num_bench)\n","_row_id_benchmark    = [[429] for i in range(10000)]\n","_method              = config['method']\n","_benchmark           = config['n_estimators']\n","_file_name           = config['file_name']\n","_debug               = config['debug']\n","\n","_n_columns_benchmark.sort()\n","\n","# Criação do objeto kmeans (ou outro método, dependendo do JSON)\n","kmeans = make_kmeans_dataset(data, thread_list, _k, _dim)\n","\n","data_original = pd.DataFrame(data)\n","\n","if not _row_id_benchmark:\n","    _row_id_benchmark =  [None for i in range(len(_n_columns_benchmark)*_num_bench)]\n","# Loop pelos parâmetros do benchmark\n","for _n_columns, _row_id in zip(_n_columns_benchmark, _row_id_benchmark):\n","    # Construção das novas colunas e medição de tempo\n","    start = time.time()\n","    new_data, index = kmeans.build_new_columns(n_columns=_n_columns, row_id=_row_id,\n","                                               method=_method, debug=_debug)\n","    end = time.time()\n","\n","    new_data = new_data.astype('float32')\n","\n","\n","    ref_string = f\"\\t\\t====Starting Random Forest Benchmark {_id_start}====\"\n","    print(ref_string)\n","\n","\n","    for _n_estimators, _n_depth in _benchmark:\n","        print(f'====n_estimators: {_n_estimators}, n_depth: {_n_depth}====')\n","        start_rf = time.time()\n","        acc = run_random_forest(new_data, target, _n_estimators, _n_depth, debug=_debug)\n","        end_rf = time.time()\n","        acc_real = run_random_forest(data_original, target, _n_estimators, _n_depth, debug=_debug)\n","        print(f\"acc: {acc}, time: {(end_rf - start_rf)}\")\n","        diff = (acc_real - acc)\n","        point = {\n","                \"id\": _id_start,\n","                'k': _k,\n","                'dim': _dim,\n","                'n_columns': _n_columns,\n","                'att': index,\n","                'method': _method,\n","                \"n_estimators\": _n_estimators,\n","                \"n_depth\": _n_depth,\n","                \"acc\": acc,\n","                \"n_nodes\": '-',\n","                \"time\": (end-start)+ (end_rf-start_rf),\n","                'diff': diff\n","        }\n","\n","\n","\n","        write_dict_to_csv(f\"./results/{_file_name}\", point)\n","\n","    target_string = \"==========================================\"\n","    formatted_string = target_string.center(len(ref_string), '=')\n","    print(formatted_string)\n","\n","\n","    _id_start += 1"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":0}